{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Distillation Overview\n",
    "\n",
    "## What is Distillation?\n",
    "Using a larger model to fine-tune a smaller model.\n",
    "\n",
    "## Why Use Distillation?\n",
    "Two primary benefits:\n",
    "1. Cost reduction\n",
    "2. Lower latency\n",
    "\n",
    "For specific tasks, we can significantly reduce both price and response time by transitioning to a smaller model.\n",
    "\n",
    "## Notebook Objectives\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Dataset analysis\n",
    "- Distillation of GPT-4O outputs to GPT-4O-mini\n",
    "- Performance comparison with non-distilled GPT-4O-mini\n",
    "- Implementation of Structured Outputs for classification\n",
    "- Impact of fine-tuning on structured output performance\n",
    "\n",
    "## Today's Agenda\n",
    "\n",
    "1. Dataset Analysis\n",
    "2. Model Comparison\n",
    "   - GPT-4O output analysis\n",
    "   - GPT-4O-mini performance baseline\n",
    "   - Performance differential highlighting\n",
    "3. Distillation Process\n",
    "   - Implementation\n",
    "   - Performance analysis of distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "\n",
    "## Data Source\n",
    "- Dataset: Wine Reviews\n",
    "- Source: Kaggle Challenge\n",
    "- Link: https://www.kaggle.com/datasets/zynicide/wine-reviews\n",
    "\n",
    "## Data Processing\n",
    "For this demonstration, we'll focus on:\n",
    "- French wines only\n",
    "- Limited subset of grape varieties\n",
    "- 500 random sample rows\n",
    "\n",
    "### Filtering Criteria\n",
    "1. Country: France only\n",
    "2. Minimum occurrences: >5 reviews per grape variety\n",
    "3. Sample size: 500 random entries\n",
    "\n",
    "## Classification Task\n",
    "Goal: Predict grape variety based on multiple features:\n",
    "- Description\n",
    "- Subregion\n",
    "- Province\n",
    "- Other available criteria\n",
    "\n",
    "### Note\n",
    "You can experiment by removing certain features to test their impact on model performance. This helps identify which contextual information provides the most value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./winemag-data-130k-v2.csv\")\n",
    "df_france = df[df[\"country\"] == \"France\"]\n",
    "\n",
    "# Let's also filter out wines that have less than 5 references with their grape variety â€“ even though we'd like to find those\n",
    "# they're outliers that we don't want to optimize for that would make our enum list be too long\n",
    "# and they could also add noise for the rest of the dataset on which we'd like to guess, eventually reducing our accuracy.\n",
    "\n",
    "varieties_less_than_five_list = (\n",
    "    df_france[\"variety\"]\n",
    "    .value_counts()[df_france[\"variety\"].value_counts() < 5]\n",
    "    .index.tolist()\n",
    ")\n",
    "df_france = df_france[~df_france[\"variety\"].isin(varieties_less_than_five_list)]\n",
    "\n",
    "df_france_subset = df_france.sample(n=500)\n",
    "#df_france_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GewÃ¼rztraminer', 'Pinot Gris', 'Gamay',\n",
       "       'Bordeaux-style White Blend', 'Champagne Blend', 'Chardonnay',\n",
       "       'Petit Manseng', 'Riesling', 'White Blend', 'Pinot Blanc',\n",
       "       'Alsace white blend', 'Bordeaux-style Red Blend', 'Malbec',\n",
       "       'Tannat-Cabernet', 'RhÃ´ne-style Red Blend', 'Ugni Blanc-Colombard',\n",
       "       'Savagnin', 'Pinot Noir', 'RosÃ©', 'Melon',\n",
       "       'RhÃ´ne-style White Blend', 'Pinot Noir-Gamay', 'Colombard',\n",
       "       'Chenin Blanc', 'Sylvaner', 'Sauvignon Blanc', 'Red Blend',\n",
       "       'Chenin Blanc-Chardonnay', 'Cabernet Sauvignon', 'Cabernet Franc',\n",
       "       'Syrah', 'Sparkling Blend', 'Duras', 'Provence red blend',\n",
       "       'Tannat', 'Merlot', 'Malbec-Merlot', 'Chardonnay-Viognier',\n",
       "       'Cabernet Franc-Cabernet Sauvignon', 'Muscat', 'Viognier',\n",
       "       'Picpoul', 'Altesse', 'Provence white blend', 'Mondeuse',\n",
       "       'Grenache-Syrah', 'G-S-M', 'Pinot Meunier', 'Cabernet-Syrah',\n",
       "       'Vermentino', 'Marsanne', 'Colombard-Sauvignon Blanc',\n",
       "       'Gros and Petit Manseng', 'JacquÃ¨re', 'Negrette', 'Mauzac',\n",
       "       'Pinot Auxerrois', 'Grenache', 'Roussanne', 'Gros Manseng',\n",
       "       'Tannat-Merlot', 'AligotÃ©', 'Chasselas', \"Loin de l'Oeil\",\n",
       "       'Malbec-Tannat', 'Carignan', 'Colombard-Ugni Blanc', 'SÃ©millon',\n",
       "       'Syrah-Grenache', 'Sciaccerellu', 'Auxerrois', 'MourvÃ¨dre',\n",
       "       'Tannat-Cabernet Franc', 'Braucol', 'Trousseau',\n",
       "       'Merlot-Cabernet Sauvignon'], dtype='<U33')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's retrieve all grape varieties to include them in the prompt and in our structured outputs enum list.\n",
    "\n",
    "varieties = np.array(df_france[\"variety\"].unique()).astype(\"str\")\n",
    "varieties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Based on this wine review, guess the grape variety:\\n    This wine is produced by Trimbach in the Alsace region of France.\\n    It was grown in Alsace. It is described as: \"This dry and restrained wine offers spice in profusion. Balanced with acidity and a firm texture, it\\'s very much for food.\".\\n    The wine has been reviewed by Roger Voss and received 87 points.\\n    The price is 24.0.\\n\\n    Here is a list of possible grape varieties to choose from: GewÃ¼rztraminer, Pinot Gris, Gamay, Bordeaux-style White Blend, Champagne Blend, Chardonnay, Petit Manseng, Riesling, White Blend, Pinot Blanc, Alsace white blend, Bordeaux-style Red Blend, Malbec, Tannat-Cabernet, RhÃ´ne-style Red Blend, Ugni Blanc-Colombard, Savagnin, Pinot Noir, RosÃ©, Melon, RhÃ´ne-style White Blend, Pinot Noir-Gamay, Colombard, Chenin Blanc, Sylvaner, Sauvignon Blanc, Red Blend, Chenin Blanc-Chardonnay, Cabernet Sauvignon, Cabernet Franc, Syrah, Sparkling Blend, Duras, Provence red blend, Tannat, Merlot, Malbec-Merlot, Chardonnay-Viognier, Cabernet Franc-Cabernet Sauvignon, Muscat, Viognier, Picpoul, Altesse, Provence white blend, Mondeuse, Grenache-Syrah, G-S-M, Pinot Meunier, Cabernet-Syrah, Vermentino, Marsanne, Colombard-Sauvignon Blanc, Gros and Petit Manseng, JacquÃ¨re, Negrette, Mauzac, Pinot Auxerrois, Grenache, Roussanne, Gros Manseng, Tannat-Merlot, AligotÃ©, Chasselas, Loin de l\\'Oeil, Malbec-Tannat, Carignan, Colombard-Ugni Blanc, SÃ©millon, Syrah-Grenache, Sciaccerellu, Auxerrois, MourvÃ¨dre, Tannat-Cabernet Franc, Braucol, Trousseau, Merlot-Cabernet Sauvignon.\\n    \\n    What is the likely grape variety? Answer only with the grape variety name or blend from the list.\\n    '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build out a function to generate our prompt and try it for the first wine of our list.\n",
    "def generate_prompt(row, varieties):\n",
    "    # Format the varieties list as a comma-separated string\n",
    "    variety_list = \", \".join(varieties)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on this wine review, guess the grape variety:\n",
    "    This wine is produced by {row['winery']} in the {row['province']} region of {row['country']}.\n",
    "    It was grown in {row['region_1']}. It is described as: \"{row['description']}\".\n",
    "    The wine has been reviewed by {row['taster_name']} and received {row['points']} points.\n",
    "    The price is {row['price']}.\n",
    "\n",
    "    Here is a list of possible grape varieties to choose from: {variety_list}.\n",
    "    \n",
    "    What is the likely grape variety? Answer only with the grape variety name or blend from the list.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Example usage with a specific row\n",
    "prompt = generate_prompt(df_france.iloc[0], varieties)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Analysis for Distillation\n",
    "\n",
    "## Cost Analysis Method\n",
    "Using tiktoken to:\n",
    "- Calculate total token count\n",
    "- Estimate completion costs\n",
    "- Evaluate distillation ROI\n",
    "\n",
    "## Cost Factors\n",
    "1. Completion Costs\n",
    "   - Based on token count\n",
    "   - Varies by model type\n",
    "2. Fine-tuning Costs (calculated separately)\n",
    "   - Depends on:\n",
    "     - Number of epochs\n",
    "     - Training set size\n",
    "     - Other training parameters\n",
    "\n",
    "### Note\n",
    "This analysis provides completion cost estimates only. The full ROI calculation must include fine-tuning costs, which we'll cover in the distillation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the dataset: 245619\n",
      "Total number of prompts: 500\n",
      "0.6140475000000001\n",
      "0.036842849999999996\n"
     ]
    }
   ],
   "source": [
    "# Load encoding for the GPT-4 model\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# Initialize a variable to store the total number of tokens\n",
    "total_tokens = 0\n",
    "\n",
    "for index, row in df_france_subset.iterrows():\n",
    "    prompt = generate_prompt(row, varieties)\n",
    "\n",
    "    # Tokenize the input text and count tokens\n",
    "    tokens = enc.encode(prompt)\n",
    "    token_count = len(tokens)\n",
    "\n",
    "    # Add the token count to the total\n",
    "    total_tokens += token_count\n",
    "\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")\n",
    "print(f\"Total number of prompts: {len(df_france_subset)}\")\n",
    "\n",
    "# outputing cost in $ as of 2024/10/23\n",
    "\n",
    "gpt4o_token_price = 2.50 / 1_000_000  # $2.50 per 1M tokens\n",
    "gpt4o_mini_token_price = 0.150 / 1_000_000  # $0.15 per 1M tokens\n",
    "\n",
    "total_gpt4o_cost = gpt4o_token_price * total_tokens\n",
    "total_gpt4o_mini_cost = gpt4o_mini_token_price * total_tokens\n",
    "\n",
    "print(total_gpt4o_cost)\n",
    "print(total_gpt4o_mini_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs Implementation\n",
    "\n",
    "## Benefits\n",
    "Structured outputs provide:\n",
    "- Deterministic responses\n",
    "- Improved accuracy\n",
    "- Restricted answer sets\n",
    "- Direct answer comparison capability\n",
    "\n",
    "## Implementation Features\n",
    "- Ensures consistent response format\n",
    "- Prevents responses outside dataset\n",
    "- Enables direct performance comparison\n",
    "- Works across all models (including distilled)\n",
    "\n",
    "### Example\n",
    "Instead of varied responses like:\n",
    "- \"I think this is Pinot Noir\"\n",
    "- \"Because of A and B, I believe this to be Pinot Noir\"\n",
    "\n",
    "We get standardized output:\n",
    "- \"Pinot Noir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"grape-variety\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"variety\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": varieties.tolist()\n",
    "                }\n",
    "            },\n",
    "            \"additionalProperties\": False,\n",
    "            \"required\": [\"variety\"],\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Implementation\n",
    "\n",
    "## Process Overview\n",
    "1. Store completions from larger model (GPT-4O)\n",
    "2. Use stored completions for smaller model fine-tuning\n",
    "3. Store all model completions for comparison\n",
    "   - GPT-4O\n",
    "   - GPT-4O-mini\n",
    "   - Fine-tuned model\n",
    "\n",
    "## Implementation Details\n",
    "### Storage Configuration\n",
    "- Enable `store=True` parameter\n",
    "- Include metadata tags for filtering\n",
    "- Support for OpenAI platform evaluations\n",
    "\n",
    "### Performance Comparison\n",
    "Initial results show:\n",
    "- GPT-4O outperforms GPT-4O-mini by 12.80%\n",
    "- Relative improvement of almost 20%\n",
    "\n",
    "## OpenAI Platform Integration\n",
    "Access stored completions at:\n",
    "https://platform.openai.com/chat-completions\n",
    "\n",
    "### Note\n",
    "Metadata tagging enables efficient filtering for:\n",
    "- Distillation processes\n",
    "- Evaluation runs\n",
    "- Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the progress index\n",
    "metadata_value = \"wine-distillation\"  # that's a funny metadata tag :-)\n",
    "\n",
    "\n",
    "# Function to call the API and process the result for a single model (blocking call in this case)\n",
    "def call_model(model, prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        store=True,\n",
    "        metadata={\n",
    "            \"distillation\": metadata_value,\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You're a sommelier expert and you know everything about wine. You answer precisely with the name of the variety/blend.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_format=response_format,\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content.strip())[\"variety\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing Implementation\n",
    "\n",
    "## Overview\n",
    "When running completions on large datasets, parallel processing becomes essential for efficiency. This section details the implementation of concurrent processing for model completions.\n",
    "\n",
    "## Implementation Features\n",
    "\n",
    "### API Call Function\n",
    "- Model-specific API calls\n",
    "- Metadata tagging for tracking\n",
    "- Structured response handling\n",
    "- System role configuration for sommelier expertise\n",
    "\n",
    "### Processing Options\n",
    "\n",
    "#### Parallel Processing\n",
    "- Uses `concurrent.futures.ThreadPoolExecutor`\n",
    "- Configurable worker count\n",
    "- Progress tracking\n",
    "- Error handling and reporting\n",
    "\n",
    "#### Sequential Processing\n",
    "- Alternative slower but ordered processing\n",
    "- Useful for debugging\n",
    "- Progress tracking included\n",
    "- Built-in error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(index, row, model, df, progress_bar):\n",
    "    global progress_index\n",
    "\n",
    "    try:\n",
    "        # Generate the prompt using the row\n",
    "        prompt = generate_prompt(row, varieties)\n",
    "\n",
    "\n",
    "\n",
    "        df.at[index, model + \"-variety\"] = call_model(model, prompt)\n",
    "\n",
    "        time.sleep(2.0)  # Adjust delay as needed\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        progress_index += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing model {model}: {str(e)}\")\n",
    "\n",
    "def process_dataframe(df, model):\n",
    "    global progress_index\n",
    "    progress_index = 1  # Reset progress index\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    with tqdm(total=len(df), desc=\"Processing rows\") as progress_bar:\n",
    "        # Process each example concurrently using ThreadPoolExecutor\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    process_example, index, row, model, df, progress_bar\n",
    "                ): index\n",
    "                for index, row in df.iterrows()\n",
    "            }\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    future.result()  # Wait for each example to be processed\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example: {str(e)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# or in order, a lot slower =(\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_dataframe_sequential(df, model):\n",
    "    global progress_index\n",
    "    progress_index = 1  # Reset progress index\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    with tqdm(total=len(df), desc=\"Processing rows\") as progress_bar:\n",
    "        # Process each example sequentially\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                process_example(index, row, model, df, progress_bar)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example at index {index}: {str(e)}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [10:50<00:00,  1.30s/it]\n",
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [10:43<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Let's try out our call model function before processing the whole dataframe and check the output.\n",
    "\n",
    "answer = call_model(\"gpt-4o\", generate_prompt(df_france_subset.iloc[0], varieties))\n",
    "answer\n",
    "\n",
    "df_france_subset = process_dataframe(df_france_subset, \"gpt-4o\")\n",
    "df_france_subset = process_dataframe(df_france_subset, \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Analysis\n",
    "\n",
    "## Comparison Framework\n",
    "We compare two models:\n",
    "- GPT-4O\n",
    "- GPT-4O-mini\n",
    "\n",
    "## Methodology\n",
    "- Direct comparison against expected grape varieties\n",
    "- Accuracy assessment using structured outputs\n",
    "- String-based verification enabled by response format\n",
    "\n",
    "### Key Findings\n",
    "- GPT-4O demonstrates superior accuracy\n",
    "- Performance delta: 12.80% absolute improvement\n",
    "- Relative improvement: ~20% compared to GPT-4O-mini\n",
    "\n",
    "### Implications\n",
    "This significant performance gap presents:\n",
    "- Clear opportunity for distillation\n",
    "- Potential for cost optimization\n",
    "- Balance between accuracy and latency\n",
    "\n",
    "## Next Steps\n",
    "The performance differential justifies exploring distillation to:\n",
    "1. Maintain high accuracy\n",
    "2. Reduce operational costs\n",
    "3. Improve response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o accuracy: 82.00%\n",
      "gpt-4o-mini accuracy: 70.00%\n"
     ]
    }
   ],
   "source": [
    "models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "\n",
    "\n",
    "def get_accuracy(model, df):\n",
    "    return np.mean(df[\"variety\"] == df[model + \"-variety\"])\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(f\"{model} accuracy: {get_accuracy(model, df_france_subset) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOW\n",
    "## We can see that gpt-4o is better a finding grape variety than 4o-mini (12.00% higher or almost 20% relatively to 4o-mini!). \n",
    "\n",
    "# Now we go to openai dashboard to fine-tune!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste your fine-tune job ID below\n",
    "finetune_job = client.fine_tuning.jobs.retrieve(\"ftjob-my-code\")\n",
    "\n",
    "if finetune_job.status == \"succeeded\":\n",
    "    fine_tuned_model = finetune_job.fine_tuned_model\n",
    "    print(\"finetuned model: \" + fine_tuned_model)\n",
    "else:\n",
    "    print(\"finetuned job status: \" + finetune_job.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the Distilled Model\n",
    "\n",
    "## Validation Process\n",
    "\n",
    "### Steps\n",
    "1. Run completions using the fine-tuned model\n",
    "2. Compare accuracy metrics:\n",
    "   - GPT-4O baseline\n",
    "   - GPT-4O-mini baseline\n",
    "   - Distilled model performance\n",
    "\n",
    "### Dataset Preparation\n",
    "For accurate validation:\n",
    "- Select new subset of French wines\n",
    "- Maintain consistency with training constraints:\n",
    "  - French grape varieties only\n",
    "  - Excluded outlier varieties\n",
    "  - Same filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = df_france.sample(n=300)\n",
    "\n",
    "models.append(fine_tuned_model)\n",
    "\n",
    "for model in models:\n",
    "    another_subset = process_dataframe(validation_dataset, model)\n",
    "\n",
    "# Let's compare accuracy of models\n",
    "\n",
    "for model in models:\n",
    "    print(f\"{model} accuracy: {get_accuracy(model, another_subset) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's almost a 22% relative improvement over the non-distilled gpt-4o-mini! ðŸŽ‰\n",
    "\n",
    "## Our fine-tuned model performs way better than gpt-4o-mini, while having the same base model!  \n",
    "\n",
    "### We'll be able to use this model to run inferences at a lower cost and lower latency for future grape variety prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shout out to openAI\n",
    "\n",
    "Distilling gpt-4o outputs to gpt-4o-mini\n",
    "Let's assume we'd like to run this prediction often, we want completions to be faster and cheaper, but keep that level of accuracy. That'd be great to be able to distill 4o accuracy to 4o-mini, wouldn't it? Let's do it!\n",
    "\n",
    "We'll now go to OpenAI Stored completions page: https://platform.openai.com/chat-completions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
